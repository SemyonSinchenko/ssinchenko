<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Working With File System from PySpark | Sem Sinchenko</title>
<meta name="keywords" content="pyspark, spark, hadoop">
<meta name="description" content="Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk.">
<meta name="author" content="Sem Sinchenko">
<link rel="canonical" href="https://semyonsinchenko.gihub.io/ssinchenko/post/working-with-fs-pyspark/">
<link crossorigin="anonymous" href="/ssinchenko/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/apple-touch-icon.png">
<link rel="mask-icon" href="https://semyonsinchenko.gihub.io/ssinchenko/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://semyonsinchenko.gihub.io/ssinchenko/post/working-with-fs-pyspark/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Working With File System from PySpark" />
<meta property="og:description" content="Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://semyonsinchenko.gihub.io/ssinchenko/post/working-with-fs-pyspark/" /><meta property="og:image" content="https://semyonsinchenko.gihub.io/ssinchenko/images/avatar-favicon.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-03-30T00:21:28+02:00" />
<meta property="article:modified_time" content="2023-03-30T00:21:28+02:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://semyonsinchenko.gihub.io/ssinchenko/images/avatar-favicon.png"/>

<meta name="twitter:title" content="Working With File System from PySpark"/>
<meta name="twitter:description" content="Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://semyonsinchenko.gihub.io/ssinchenko/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Working With File System from PySpark",
      "item": "https://semyonsinchenko.gihub.io/ssinchenko/post/working-with-fs-pyspark/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Working With File System from PySpark",
  "name": "Working With File System from PySpark",
  "description": "Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk.",
  "keywords": [
    "pyspark", "spark", "hadoop"
  ],
  "articleBody": " Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk.\nBut PySpark applications are running in cluster mode, especially in a production environment. And all that we have is some distributed file system or object storage like HDFS, S3, Azure Blob Storage, etc.\nRegular approach An obvious solution is of course to use some side library. For example, we can use boto3 for working with S3, pyarrow for working with HDFS, or built-in Pathlib for Local One. But there are some problems:\nSometimes it is a bad idea to take a huge library and add it to the project as a dependency especially if all that we need is just read or write some bytes from/to storage; All of these libraries has own abstractions and interfaces. So each user should learn one more API; Sometimes we need to be able to write into Local Files System when running unit tests but into some cloud storage from production. Of course one can use unittest.mock.patch (or pytest fixtures) but it can make writing tests not a trivial task. At the same moment, we know that PySpark can read and write data quite effectively into any file system. Moreover, spark understands which system is it by path prefix. For example in this code we shouldn't specify the file system, all we need is just write the right prefix:\nspark_data_frame.write.parquet(\"s3a://my-prefix/table\") # this will write to S3 bucket spark_data_frame.write.parquet(\"file://my-home-dir/table\") # and this one will save data locally So why not use such built-in `PySpark` features?\nJava way Spark is written in Scala, a language from the JVM family. And under the hood Spark steel heavily uses org.apache.hadoop so this jar is accessible out-of-the-box in almost each Spark setup. We can make a look into the documentation of org.apache.hadoop.fs.FileSystem: a main class for making i/o operations. There are implementations for S3, HDFS, Local and Azure file storage. So we can use a single interface and all the advantages of Java classes hierarchy and do not care about which implementation to use where. Imagine we have a SparkSession in some Java program. In this case, we can write code like this:\nimport org.apache.spark.sql.SparkSession; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; public class Main { public static void main(String[] args) { SparkSession spark = SparkSession.builder.getOrCreate(); FileSystem fs = FileSystem.get(spark.sparkContext.hadoopConfiguration); Path oldFile = new Path(\"hdfs://some-place/old-file\"); Path newFile = new Path(\"hdfs://some-place/new-file\"); fs.rename(oldFile, newFile); } } Here we used, for example, method of FileSystem. Generally speaking, we can do any file operation (move, read, write, rename, glob, etc.) with such a class. For Scala users there is also a nice scala library with a simple, functional interface that hide FileSystem under the hood and provides clean public interfaces.\nBut how can we use this solution from PySpark?\nPySpark solution Working with py4j and JVM Interestingly, all the PySpark is built on the shoulders of py4j: a library with 1100 stars in GitHub. Just for comparison spark has 35300 stars on GitHub.\nUnder the hood PySpark just wraps Scala calls into py4j. In spark runtime you have access to JVM:\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() spark_jvm = spark.sparkContext._jvm Let's create some Java objects and try to interact with them from Python.\njava_int = spark_jvm.java.lang.Integer(1) java_another_int = spark_jvm.java.lang.Integer(2) print(java_int + java_another_int) 3 Under the hood py4j implicitly make the conversion from simple Python types into simple Java types. In the example above we pass python int into java.lang.Integer as is. We can do the same things with strings, numbers, and sometimes with lists. But often we should explicitly covert types from python to Java and back.\nCreate a FileSystem instance Let's create a FileSystem instance. From the documentation, we can see that there is a constructor (constructor in Java is like __init__(self, **kwargs) in Python) but it is protected which means it is accessible only from FileSystem class but not from outside. But there are few static methods that allows us to initialize an instance of FileSystem:\nget(Configuration conf) Returns the configured FileSystem implementation. get(URI uri, Configuration conf) Get a FileSystem for this URI's scheme and authority. get(URI uri, Configuration conf, String user) Get a FileSystem instance based on the uri, the passed in configuration and the user. At first, we need to get Configuration conf instance which contains all the information about FileSystem. The good news is that we can get it from our SparkSession object directly from python:\nhadoop_conf = spark._jsc.hadoopConfiguration() Here we are using another object: jsc which is the same SparkContext but accessible not via pyspark wrapper but as JavaObject.\nTo allow spark to choose the right implementation of FileSystem (for example, NativeS3FileSystem for S3 or RawLocalFileSystem for local files) we should pass into get method also URI. To get a URI from a simple path we can use org.apache.hadoop.fs.Path.toUri method:\ndef _get_hdfs( spark: SparkSession, pattern: str ) -\u003e Tuple[JavaObject, JavaObject]: # Java is accessible in runtime only and it is impossible to infer types here hadoop = spark.sparkContext._jvm.org.apache.hadoop # type: ignore hadoop_conf = spark._jsc.hadoopConfiguration() # type: ignore uri = hadoop.fs.Path(pattern).toUri() # type: ignore hdfs = hadoop.fs.FileSystem.get(uri, hadoop_conf) # type: ignore return (hadoop, hdfs) # type: ignore This function gets a spark session and a pattern (or path) and returns us hadoop and FileSystem instance based on the given SparkSession. So if you, for example, already configure your spark session to work with S3 such a function will use this configuration.\nList files The simplest operation we can do with such an instance of FileSystem is to list files in a distributed or local file system. It is sometimes very useful for example if we check if some path exists or to find some directories based on a pattern. There is a method public FileStatus[] globStatus(Path pathPattern) which takes a pattern and returns Java array of FileStatus objects. Let's see how it works:\nhadoop = spark.sparkContext._jvm.org.apache.hadoop # syntax sugar for simplifying the code path = hadoop.fs.Path(\"file:///home/sem/*\") hdfs = hadoop.fs.FileSystem.get(path.toUri(), spark._jsc.hadoopConfiguration()) statuses = file_system.globStatus(path) print(len(statuses)) 105 What happens if we pass a wrong path? hadoop = spark.sparkContext._jvm.org.apache.hadoop # syntax sugar for simplifying the code path = hadoop.fs.Path(\"file://home/sem/*\") hdfs = hadoop.fs.FileSystem.get(path.toUri(), spark._jsc.hadoopConfiguration()) statuses = file_system.globStatus(path) print(len(statuses)) pyspark.sql.utils.IllegalArgumentException: Wrong FS: file://home/sem, expected: file:/// Working with FileStatus To provide a top-level python API we should convert results of globStatus from Java FileStatus[] into python list. To do it lets create a data container for storing information about files:\n@dataclass class HDFSFile: name: str path: str mod_time: int is_dir: bool After that we can loop through statuses and extract information from Java objects to store it inside dataclasses:\nres = [] for file_status in statuses: res.append( HDFSFile( name=file_status.getPath().getName(), path=file_status.getPath().toString(), mod_time=file_status.getModificationTime(), is_dir=file_status.isDirectory(), ) ) Working with strings The next thing we want to have here is the ability to write and read strings. Using just simple strings we can serialize a lot of objects into, for example, json and yaml format. But here we are facing some problems. If we make a look into the documentation of FileSystem we find that the main way to write information is a FSDataOutputStream (link to the documentation). It implements a DataOutputStream abstraction which provides two methods that look interesting from the first view:\npublic final void writeUTF(String str) public final void writeChars(String s) Unfortunately both of them have very bad compatibility with Python UTF-8 strings. The first one uses modified UTF-8 which is useful if you need to have C compatibility but such strings are unreadable from python side (you can read them only as bytes and after that manually decode them). The second one uses UTF-16BE encoding which is some kind of standard in Java but also cannot be simply read as string from Python.\npath = hadoop.fs.Path(\"file:///home/sem/test_file.txt\") output_stream = file_system.create(path) output_stream.writeChars(\"some testing data with utf-8 symbols: Ð°Ð±Ð²Ð³Ð´ÐµÐ¶ðŸ˜Š\") output_stream.flush() output_stream.close() with open(\"/home/sem/test_file.txt\", \"r\") as test_file: print(test_file.read()) (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd8 in position 90: invalid continuation byte Of course, you are still able to read the data as bytes and decode it manually:\nwith open(\"/home/sem/test_file.txt\", \"br\") as byte_file: print(byte_file.read().decode(\"utf-16be\")) some testing data with utf-8 symbols: Ð°Ð±Ð²Ð³Ð´ÐµÐ¶ðŸ˜Š But it is not the better option. A better way is to write data as bytes on the Java side but read it as regular a string on python side:\ndef write_utf8( hdfs, hadoop, path: str, data: str, mode: Literal[\"a\", \"w\"] ) -\u003e None: \"\"\"Write a given string in UTF-16BE to the given path. Do not use this method to write the data! It is fantastically slow compared to `spark.write`. :param path: Path of file :param data: String to write :param mode: Mode. `w` means overwrite but `a` means append. \"\"\" if mode == \"w\": # org.apache.hadoop.fs.FileSystem.create(Path f, boolean overwrite) output_stream = hdfs.create(hadoop.fs.Path(path), True) # type: ignore elif mode == \"a\": # org.apache.hadoop.fs.FileSystem.append(Path f) output_stream = hdfs.append(hadoop.fs.Path(path)) # type: ignore # org.apache.hadoop.fs.FSDataOutputStream try: for b in data.encode(\"utf-8\"): output_stream.write(b) output_stream.flush() output_stream.close() except Exception as e: output_stream.close() raise e Combining all together Finally, we are ready to combine it all together and create a class for working with File Systems when all these py4 things are hidden under the hood.\nimport enum import re from dataclasses import dataclass from typing import List, Literal, Tuple from py4j.java_gateway import JavaObject from pyspark.sql import SparkSession _FS_PATTERN = r\"(s3\\w*://|hdfs://|dbfs://|file://|file:/).(.*)\" class FS_TYPES(enum.Enum): DBFS = \"DBFS\" HDFS = \"HDFS\" S3 = \"S3\" LOCAL = \"LOCAL\" UNKNOWN = \"UNKNOWN\" @classmethod def _from_pattern(cls, pattern: str): return { \"s3://\": cls.S3, \"s3a://\": cls.S3, \"dbfs://\": cls.DBFS, \"hdfs://\": cls.HDFS, \"file://\": cls.LOCAL, }.get(pattern, cls.UNKNOWN) @dataclass class HDFSFile: name: str path: str mod_time: int is_dir: bool fs_type: FS_TYPES def _get_hdfs( spark: SparkSession, pattern: str ) -\u003e Tuple[JavaObject, JavaObject, FS_TYPES]: match = re.match(_FS_PATTERN, pattern) if match is None: raise ValueError( f\"Bad pattern or path. Got {pattern} but should be\" \" one of `s3://`, `s3a://`, `dbfs://`, `hdfs://`, `file://`\" ) fs_type = FS_TYPES._from_pattern(match.groups()[0]) # Java is accessible in runtime only and it is impossible to infer types here hadoop = spark.sparkContext._jvm.org.apache.hadoop # type: ignore hadoop_conf = spark._jsc.hadoopConfiguration() # type: ignore uri = hadoop.fs.Path(pattern).toUri() # type: ignore hdfs = hadoop.fs.FileSystem.get(uri, hadoop_conf) # type: ignore return (hadoop, hdfs, fs_type) # type: ignore class HadoopFileSystem(object): def __init__(self: \"HadoopFileSystem\", spark: SparkSession, pattern: str) -\u003e None: \"\"\"Helper class for working with FileSystem. :param spark: SparkSession object :param pattern: Any pattern related to FileSystem. We should provide it to choose the right implementation of org.apache.hadoop.fs.FileSystem under the hood. Pattern here should have a form of URI-like string like `s3a:///my-bucket/my-prefix` or `file:///home/user/`. \"\"\" hadoop, hdfs, fs_type = _get_hdfs(spark, pattern) self._hdfs = hdfs self._fs_type = fs_type self._hadoop = hadoop self._jvm = spark.sparkContext._jvm def write_utf8( self: \"HadoopFileSystem\", path: str, data: str, mode: Literal[\"a\", \"w\"] ) -\u003e None: \"\"\"Write a given string in UTF-16BE to the given path. Do not use this method to write the data! It is fantastically slow compared to `spark.write`. :param path: Path of file :param data: String to write :param mode: Mode. `w` means overwrite but `a` means append. \"\"\" if mode == \"w\": # org.apache.hadoop.fs.FileSystem.create(Path f, boolean overwrite) output_stream = self._hdfs.create(self._hadoop.fs.Path(path), True) # type: ignore elif mode == \"a\": # org.apache.hadoop.fs.FileSystem.append(Path f) output_stream = self._hdfs.append(self._hadoop.fs.Path(path)) # type: ignore # org.apache.hadoop.fs.FSDataOutputStream try: for b in data.encode(\"utf-8\"): output_stream.write(b) output_stream.flush() output_stream.close() except Exception as e: output_stream.close() raise e def read_utf8(self: \"HadoopFileSystem\", path: str) -\u003e str: \"\"\"Read string from given path. Do not use this method to read the data! It is fantastically slow compared to `spark.read`. :param path: Path of file :return: Decoded from UTF-8 string :rtype: str \"\"\" res = [] # org.apache.hadoop.fs.FileSystem.open in_stream = self._hdfs.open(self._hadoop.fs.Path(path)) # type: ignore # open returns us org.apache.hadoop.fs.FSDataInputStream try: while True: if in_stream.available() \u003e 0: res.append(in_stream.readByte()) else: in_stream.close() break except Exception as e: in_stream.close() raise e return bytes(res).decode(\"utf-8\") def glob(self, pattern: str) -\u003e List[HDFSFile]: statuses = self._hdfs.globStatus(self._hadoop.fs.Path(pattern)) res = [] for file_status in statuses: # org.apache.hadoop.fs.FileStatus res.append( HDFSFile( name=file_status.getPath().getName(), path=file_status.getPath().toString(), mod_time=file_status.getModificationTime(), is_dir=file_status.isDirectory(), fs_type=self._fs_type, ) ) return res Conclusion There is a nice lightweight Python library with zero additional dependencies: Eren. This library contains a lot of useful routines for working with Hive and Hadoop. I pushed the code above into this library so you are free to use it. All that you need is just to write:\nfrom eren import fs hdfs = fs.HadoopFileSystem(spark_session, \"hdfs://some-place\") s3fs = fs.HadoopFileSystem(spark_session, \"s3a://prefix/bucket\") local_fs = fs.HadoopFileSystem(spark_session, \"file://my-home-folder\") ",
  "wordCount" : "2082",
  "inLanguage": "en",
  "datePublished": "2023-03-30T00:21:28+02:00",
  "dateModified": "2023-03-30T00:21:28+02:00",
  "author":{
    "@type": "Person",
    "name": "Sem Sinchenko"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://semyonsinchenko.gihub.io/ssinchenko/post/working-with-fs-pyspark/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sem Sinchenko",
    "logo": {
      "@type": "ImageObject",
      "url": "https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://semyonsinchenko.gihub.io/ssinchenko/" accesskey="h" title="Sem Sinchenko (Alt + H)">Sem Sinchenko</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/page/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/page/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Working With File System from PySpark
    </h1>
    <div class="post-meta"><span title='2023-03-30 00:21:28 +0200 CEST'>March 30, 2023</span>&nbsp;Â·&nbsp;10 min&nbsp;Â·&nbsp;Sem Sinchenko

</div>
  </header> 
  <div class="post-content">
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Working with File System from PySpark
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<div id="outline-container-headline-2" class="outline-3">
<h3 id="headline-2">
Motivation
</h3>
<div id="outline-text-headline-2" class="outline-text-3">
<p>
Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically <code class="verbatim">json</code> or <code class="verbatim">yaml</code> files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like <code class="verbatim">matplotlib</code> plot, into bytes and write them to the disk.</p>
<p>
But PySpark applications are running in cluster mode, especially in a production environment. And all that we have is some distributed file system or object storage like HDFS, S3, Azure Blob Storage, etc.</p>
</div>
</div>
<div id="outline-container-headline-3" class="outline-3">
<h3 id="headline-3">
Regular approach
</h3>
<div id="outline-text-headline-3" class="outline-text-3">
<p>
An obvious solution is of course to use some side library. For example, we can use <code class="verbatim">boto3</code> for working with <code class="verbatim">S3</code>, <code class="verbatim">pyarrow</code> for working with <code class="verbatim">HDFS</code>, or built-in <code class="verbatim">Pathlib</code> for Local One. But there are some problems:</p>
<ol>
<li>Sometimes it is a bad idea to take a huge library and add it to the project as a dependency especially if all that we need is just read or write some bytes from/to storage;</li>
<li>All of these libraries has own abstractions and interfaces. So each user should learn one more API;</li>
<li>Sometimes we need to be able to write into Local Files System when running unit tests but into some cloud storage from production. Of course one can use <code class="verbatim">unittest.mock.patch</code> (or <code class="verbatim">pytest</code> fixtures) but it can make writing tests not a trivial task.</li>
</ol>
<p>At the same moment, we know that <code class="verbatim">PySpark</code> can read and write data quite effectively into any file system. Moreover, spark understands which system is it by path prefix. For example in this code we shouldn&#39;t specify the file system, all we need is just write the right prefix:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">spark_data_frame</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&#34;s3a://my-prefix/table&#34;</span><span class="p">)</span> <span class="c1"># this will write to S3 bucket</span>
</span></span><span class="line"><span class="cl">  <span class="n">spark_data_frame</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&#34;file://my-home-dir/table&#34;</span><span class="p">)</span> <span class="c1"># and this one will save data locally</span></span></span></code></pre></div>
</div>
<p>
So why not use such built-in `PySpark` features?</p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-3">
<h3 id="headline-4">
Java way
</h3>
<div id="outline-text-headline-4" class="outline-text-3">
<p>
Spark is written in Scala, a language from the JVM family. And under the hood Spark steel heavily uses <code class="verbatim">org.apache.hadoop</code> so this jar is accessible out-of-the-box in almost each Spark setup. We can make a look into the <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html">documentation</a> of <code class="verbatim">org.apache.hadoop.fs.FileSystem</code>: a main class for making i/o operations. There are implementations for <code class="verbatim">S3</code>, <code class="verbatim">HDFS</code>, <code class="verbatim">Local</code> and <code class="verbatim">Azure</code> file storage. So we can use a single interface and all the advantages of Java classes hierarchy and do not care about which implementation to use where. Imagine we have a <code class="verbatim">SparkSession</code> in some Java program. In this case, we can write code like this:</p>
<div class="src src-java">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="w">  </span><span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.spark.sql.SparkSession</span><span class="p">;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.hadoop.fs.FileSystem</span><span class="p">;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.hadoop.fs.Path</span><span class="p">;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="kd">public</span><span class="w"> </span><span class="kd">class</span> <span class="nc">Main</span><span class="w"> </span><span class="p">{</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="kd">public</span><span class="w"> </span><span class="kd">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">String</span><span class="o">[]</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="n">SparkSession</span><span class="w"> </span><span class="n">spark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SparkSession</span><span class="p">.</span><span class="na">builder</span><span class="p">.</span><span class="na">getOrCreate</span><span class="p">();</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="n">FileSystem</span><span class="w"> </span><span class="n">fs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FileSystem</span><span class="p">.</span><span class="na">get</span><span class="p">(</span><span class="n">spark</span><span class="p">.</span><span class="na">sparkContext</span><span class="p">.</span><span class="na">hadoopConfiguration</span><span class="p">);</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="n">Path</span><span class="w"> </span><span class="n">oldFile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Path</span><span class="p">(</span><span class="s">&#34;hdfs://some-place/old-file&#34;</span><span class="p">);</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="n">Path</span><span class="w"> </span><span class="n">newFile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Path</span><span class="p">(</span><span class="s">&#34;hdfs://some-place/new-file&#34;</span><span class="p">);</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="n">fs</span><span class="p">.</span><span class="na">rename</span><span class="p">(</span><span class="n">oldFile</span><span class="p">,</span><span class="w"> </span><span class="n">newFile</span><span class="p">);</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="p">}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="p">}</span></span></span></code></pre></div>
</div>
<p>
Here we used, for example, <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html#rename-org.apache.hadoop.fs.Path-org.apache.hadoop.fs.Path-">method</a> of <code class="verbatim">FileSystem</code>. Generally speaking, we can do any file operation (move, read, write, rename, glob, etc.) with such a class. For <code class="verbatim">Scala</code> users there is also a nice <a href="https://github.com/brayanjuls/hio">scala library</a> with a simple, functional interface that hide <code class="verbatim">FileSystem</code> under the hood and provides clean public interfaces.</p>
<p>
But how can we use this solution from PySpark?</p>
</div>
</div>
<div id="outline-container-headline-5" class="outline-3">
<h3 id="headline-5">
PySpark solution
</h3>
<div id="outline-text-headline-5" class="outline-text-3">
<div id="outline-container-headline-6" class="outline-4">
<h4 id="headline-6">
Working with py4j and JVM
</h4>
<div id="outline-text-headline-6" class="outline-text-4">
<blockquote>
<p>Interestingly, all the PySpark is built on the shoulders of py4j: a library with 1100 stars in GitHub. Just for comparison spark has 35300 stars on GitHub.</p>
</blockquote>
<p>
Under the hood <code class="verbatim">PySpark</code> just wraps Scala calls into <code class="verbatim">py4j</code>. In spark runtime you have access to <code class="verbatim">JVM</code>:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">spark_jvm</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_jvm</span></span></span></code></pre></div>
</div>
<p>
Let&#39;s create some Java objects and try to interact with them from Python.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">java_int</span> <span class="o">=</span> <span class="n">spark_jvm</span><span class="o">.</span><span class="n">java</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">Integer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">java_another_int</span> <span class="o">=</span> <span class="n">spark_jvm</span><span class="o">.</span><span class="n">java</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">Integer</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="nb">print</span><span class="p">(</span><span class="n">java_int</span> <span class="o">+</span> <span class="n">java_another_int</span><span class="p">)</span></span></span></code></pre></div>
</div>
<div class="src src-shell">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="m">3</span></span></span></code></pre></div>
</div>
<blockquote>
<p>Under the hood py4j implicitly make the conversion from simple Python types into simple Java types. In the example above we pass python int into <code class="verbatim">java.lang.Integer</code> as is. We can do the same things with strings, numbers, and sometimes with lists. But often we should explicitly covert types from python to Java and back.</p>
</blockquote>
</div>
</div>
<div id="outline-container-headline-7" class="outline-4">
<h4 id="headline-7">
Create a FileSystem instance
</h4>
<div id="outline-text-headline-7" class="outline-text-4">
<p>
Let&#39;s create a <code class="verbatim">FileSystem</code> instance. From the documentation, we can see that there is a constructor (constructor in Java is like <code class="verbatim">__init__(self, **kwargs)</code> in Python) but it is <code class="verbatim">protected</code> which means it is accessible only from <code class="verbatim">FileSystem</code> class but not from outside. But there are few <code class="verbatim">static</code> methods that allows us to initialize an instance of <code class="verbatim">FileSystem</code>:</p>
<table>
<tbody>
<tr>
<td><code class="verbatim">get(Configuration conf)</code></td>
<td>Returns the configured FileSystem implementation.</td>
</tr>
<tr>
<td><code class="verbatim">get(URI uri, Configuration conf)</code></td>
<td>Get a FileSystem for this URI&#39;s scheme and authority.</td>
</tr>
<tr>
<td><code class="verbatim">get(URI uri, Configuration conf, String user)</code></td>
<td>Get a FileSystem instance based on the uri, the passed in configuration and the user.</td>
</tr>
</tbody>
</table>
<p>
At first, we need to get <code class="verbatim">Configuration conf</code> instance which contains all the information about FileSystem. The good news is that we can get it from our <code class="verbatim">SparkSession</code> object directly from python:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">hadoop_conf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span></span></span></code></pre></div>
</div>
<blockquote>
<p>Here we are using another object: jsc which is the same SparkContext but accessible not via pyspark wrapper but as JavaObject.</p>
</blockquote>
<p>
To allow spark to choose the right implementation of FileSystem (for example, <code class="verbatim">NativeS3FileSystem</code> for S3 or <code class="verbatim">RawLocalFileSystem</code> for local files) we should pass into <code class="verbatim">get</code> method also <code class="verbatim">URI</code>. To get a <code class="verbatim">URI</code> from a simple path we can use <code class="verbatim">org.apache.hadoop.fs.Path.toUri</code> <a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/Path.html#toUri--">method</a>:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_get_hdfs</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">JavaObject</span><span class="p">,</span> <span class="n">JavaObject</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Java is accessible in runtime only and it is impossible to infer types here</span>
</span></span><span class="line"><span class="cl">  <span class="n">hadoop</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hadoop</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">  <span class="n">hadoop_conf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">  <span class="n">uri</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span><span class="o">.</span><span class="n">toUri</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">  <span class="n">hdfs</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">FileSystem</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">uri</span><span class="p">,</span> <span class="n">hadoop_conf</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="p">(</span><span class="n">hadoop</span><span class="p">,</span> <span class="n">hdfs</span><span class="p">)</span>  <span class="c1"># type: ignore</span></span></span></code></pre></div>
</div>
<p>
This function gets a spark session and a pattern (or path) and returns us hadoop and <code class="verbatim">FileSystem</code> instance based on the given SparkSession. So if you, for example, already configure your spark session to work with S3 such a function will use this configuration.</p>
</div>
</div>
<div id="outline-container-headline-8" class="outline-4">
<h4 id="headline-8">
List files
</h4>
<div id="outline-text-headline-8" class="outline-text-4">
<p>
The simplest operation we can do with such an instance of <code class="verbatim">FileSystem</code> is to list files in a distributed or local file system. It is sometimes very useful for example if we check if some path exists or to find some directories based on a pattern. There is a method <code class="verbatim">public FileStatus[] globStatus(Path pathPattern)</code> which takes a pattern and returns Java array of <code class="verbatim">FileStatus</code> objects. Let&#39;s see how it works:</p>
<div class="src src-python results: output">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">  hadoop = spark.sparkContext._jvm.org.apache.hadoop # syntax sugar for simplifying the code
</span></span><span class="line"><span class="cl">  path = hadoop.fs.Path(&#34;file:///home/sem/*&#34;)
</span></span><span class="line"><span class="cl">  hdfs = hadoop.fs.FileSystem.get(path.toUri(), spark._jsc.hadoopConfiguration())
</span></span><span class="line"><span class="cl">  statuses = file_system.globStatus(path)
</span></span><span class="line"><span class="cl">  print(len(statuses))</span></span></code></pre></div>
</div>
<div class="src src-shell">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="m">105</span></span></span></code></pre></div>
</div>
<div id="outline-container-headline-9" class="outline-5">
<h5 id="headline-9">
What happens if we pass a wrong path?
</h5>
<div id="outline-text-headline-9" class="outline-text-5">
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">hadoop</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hadoop</span> <span class="c1"># syntax sugar for simplifying the code</span>
</span></span><span class="line"><span class="cl">  <span class="n">path</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s2">&#34;file://home/sem/*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">hdfs</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">FileSystem</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">toUri</span><span class="p">(),</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="n">statuses</span> <span class="o">=</span> <span class="n">file_system</span><span class="o">.</span><span class="n">globStatus</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">statuses</span><span class="p">))</span></span></span></code></pre></div>
</div>
<div class="src src-shell">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">  pyspark.sql.utils.IllegalArgumentException: Wrong FS: file://home/sem, expected: file:///</span></span></code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-10" class="outline-5">
<h5 id="headline-10">
Working with FileStatus
</h5>
<div id="outline-text-headline-10" class="outline-text-5">
<p>
To provide a top-level python API we should convert results of <code class="verbatim">globStatus</code> from Java <code class="verbatim">FileStatus[]</code> into python <code class="verbatim">list</code>. To do it lets create a data container for storing information about files:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HDFSFile</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
</span></span><span class="line"><span class="cl">  <span class="n">path</span><span class="p">:</span> <span class="nb">str</span>
</span></span><span class="line"><span class="cl">  <span class="n">mod_time</span><span class="p">:</span> <span class="nb">int</span>
</span></span><span class="line"><span class="cl">  <span class="n">is_dir</span><span class="p">:</span> <span class="nb">bool</span></span></span></code></pre></div>
</div>
<p>
After that we can loop through statuses and extract information from Java objects to store it inside dataclasses:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">file_status</span> <span class="ow">in</span> <span class="n">statuses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="n">HDFSFile</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">getPath</span><span class="p">()</span><span class="o">.</span><span class="n">getName</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">        <span class="n">path</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">getPath</span><span class="p">()</span><span class="o">.</span><span class="n">toString</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">        <span class="n">mod_time</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">getModificationTime</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">        <span class="n">is_dir</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">isDirectory</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">      <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span></span></span></code></pre></div>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-11" class="outline-4">
<h4 id="headline-11">
Working with strings
</h4>
<div id="outline-text-headline-11" class="outline-text-4">
<p>
The next thing we want to have here is the ability to write and read strings. Using just simple strings we can serialize a lot of objects into, for example, <code class="verbatim">json</code> and  <code class="verbatim">yaml</code> format. But here we are facing some problems. If we make a look into the documentation of <code class="verbatim">FileSystem</code> we find that the main way to write information is a <code class="verbatim">FSDataOutputStream</code> (<a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FSDataOutputStream.html">link to the documentation</a>). It implements a <code class="verbatim">DataOutputStream</code> abstraction which provides two methods that look interesting from the first view:</p>
<ol>
<li><code class="verbatim">public final void writeUTF(String str)</code></li>
<li><code class="verbatim">public final void writeChars(String s)</code></li>
</ol>
<p>Unfortunately both of them have very bad compatibility with Python UTF-8 strings. The first one uses <a href="https://docs.oracle.com/javase/8/docs/api/java/io/DataInput.html#modified-utf-8">modified UTF-8</a> which is useful if you need to have <code class="verbatim">C</code> compatibility but such strings are unreadable from python side (you can read them only as bytes and after that manually decode them). The second one uses <code class="verbatim">UTF-16BE</code> encoding which is some kind of standard in Java but also cannot be simply read as string from Python.</p>
<div class="src src-python results: output">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">  path = hadoop.fs.Path(&#34;file:///home/sem/test_file.txt&#34;)
</span></span><span class="line"><span class="cl">  output_stream = file_system.create(path)
</span></span><span class="line"><span class="cl">  output_stream.writeChars(&#34;some testing data with utf-8 symbols: Ð°Ð±Ð²Ð³Ð´ÐµÐ¶ðŸ˜Š&#34;)
</span></span><span class="line"><span class="cl">  output_stream.flush()
</span></span><span class="line"><span class="cl">  output_stream.close()
</span></span><span class="line"><span class="cl">  with open(&#34;/home/sem/test_file.txt&#34;, &#34;r&#34;) as test_file:
</span></span><span class="line"><span class="cl">   print(test_file.read())</span></span></code></pre></div>
</div>
<div class="src src-shell">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">    <span class="o">(</span>result, consumed<span class="o">)</span> <span class="o">=</span> self._buffer_decode<span class="o">(</span>data, self.errors, final<span class="o">)</span>
</span></span><span class="line"><span class="cl">UnicodeDecodeError: <span class="s1">&#39;utf-8&#39;</span> codec can<span class="err">&#39;</span>t decode byte 0xd8 in position 90: invalid continuation byte</span></span></code></pre></div>
</div>
<p>
Of course, you are still able to read the data as bytes and decode it manually:</p>
<div class="src src-python results: output">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">  with open(&#34;/home/sem/test_file.txt&#34;, &#34;br&#34;) as byte_file:
</span></span><span class="line"><span class="cl">   print(byte_file.read().decode(&#34;utf-16be&#34;))</span></span></code></pre></div>
</div>
<div class="src src-shell">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">some testing data with utf-8 symbols: Ð°Ð±Ð²Ð³Ð´ÐµÐ¶ðŸ˜Š</span></span></code></pre></div>
</div>
<p>
But it is not the better option. A better way is to write data as bytes on the Java side but read it as regular a string on python side:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">write_utf8</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">hdfs</span><span class="p">,</span> <span class="n">hadoop</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&#34;a&#34;</span><span class="p">,</span> <span class="s2">&#34;w&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Write a given string in UTF-16BE to the given path.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Do not use this method to write the data!
</span></span></span><span class="line"><span class="cl"><span class="s2">    It is fantastically slow compared to `spark.write`.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param path: Path of file
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param data: String to write
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param mode: Mode. `w` means overwrite but `a` means append.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;w&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># org.apache.hadoop.fs.FileSystem.create(Path f, boolean overwrite)</span>
</span></span><span class="line"><span class="cl">      <span class="n">output_stream</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;a&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># org.apache.hadoop.fs.FileSystem.append(Path f)</span>
</span></span><span class="line"><span class="cl">      <span class="n">output_stream</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># org.apache.hadoop.fs.FSDataOutputStream</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&#34;utf-8&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">          <span class="n">output_stream</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">output_stream</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      <span class="n">output_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">output_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      <span class="k">raise</span> <span class="n">e</span></span></span></code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-12" class="outline-4">
<h4 id="headline-12">
Combining all together
</h4>
<div id="outline-text-headline-12" class="outline-text-4">
<p>
Finally, we are ready to combine it all together and create a class for working with File Systems when all these <code class="verbatim">py4</code> things are hidden under the hood.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="kn">import</span> <span class="nn">enum</span>
</span></span><span class="line"><span class="cl">  <span class="kn">import</span> <span class="nn">re</span>
</span></span><span class="line"><span class="cl">  <span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
</span></span><span class="line"><span class="cl">  <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Tuple</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="kn">import</span> <span class="n">JavaObject</span>
</span></span><span class="line"><span class="cl">  <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">_FS_PATTERN</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(s3\w*://|hdfs://|dbfs://|file://|file:/).(.*)&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">class</span> <span class="nc">FS_TYPES</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">DBFS</span> <span class="o">=</span> <span class="s2">&#34;DBFS&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">HDFS</span> <span class="o">=</span> <span class="s2">&#34;HDFS&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">S3</span> <span class="o">=</span> <span class="s2">&#34;S3&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOCAL</span> <span class="o">=</span> <span class="s2">&#34;LOCAL&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">UNKNOWN</span> <span class="o">=</span> <span class="s2">&#34;UNKNOWN&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@classmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_from_pattern</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;s3://&#34;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">S3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;s3a://&#34;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">S3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;dbfs://&#34;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">DBFS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;hdfs://&#34;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">HDFS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;file://&#34;</span><span class="p">:</span> <span class="bp">cls</span><span class="o">.</span><span class="n">LOCAL</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl">  <span class="k">class</span> <span class="nc">HDFSFile</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
</span></span><span class="line"><span class="cl">    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span>
</span></span><span class="line"><span class="cl">    <span class="n">mod_time</span><span class="p">:</span> <span class="nb">int</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_dir</span><span class="p">:</span> <span class="nb">bool</span>
</span></span><span class="line"><span class="cl">    <span class="n">fs_type</span><span class="p">:</span> <span class="n">FS_TYPES</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">_get_hdfs</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">JavaObject</span><span class="p">,</span> <span class="n">JavaObject</span><span class="p">,</span> <span class="n">FS_TYPES</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">    <span class="k">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="k">match</span><span class="p">(</span><span class="n">_FS_PATTERN</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="k">match</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="sa">f</span><span class="s2">&#34;Bad pattern or path. Got </span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s2"> but should be&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34; one of `s3://`, `s3a://`, `dbfs://`, `hdfs://`, `file://`&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">fs_type</span> <span class="o">=</span> <span class="n">FS_TYPES</span><span class="o">.</span><span class="n">_from_pattern</span><span class="p">(</span><span class="k">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Java is accessible in runtime only and it is impossible to infer types here</span>
</span></span><span class="line"><span class="cl">    <span class="n">hadoop</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hadoop</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">    <span class="n">hadoop_conf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">    <span class="n">uri</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span><span class="o">.</span><span class="n">toUri</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">    <span class="n">hdfs</span> <span class="o">=</span> <span class="n">hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">FileSystem</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">uri</span><span class="p">,</span> <span class="n">hadoop_conf</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span><span class="n">hadoop</span><span class="p">,</span> <span class="n">hdfs</span><span class="p">,</span> <span class="n">fs_type</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">class</span> <span class="nc">HadoopFileSystem</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="s2">&#34;HadoopFileSystem&#34;</span><span class="p">,</span> <span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;&#34;&#34;Helper class for working with FileSystem.
</span></span></span><span class="line"><span class="cl"><span class="s2">      :param spark: SparkSession object
</span></span></span><span class="line"><span class="cl"><span class="s2">      :param pattern: Any pattern related to FileSystem.
</span></span></span><span class="line"><span class="cl"><span class="s2">                      We should provide it to choose the right implementation of org.apache.hadoop.fs.FileSystem under the hood.
</span></span></span><span class="line"><span class="cl"><span class="s2">                      Pattern here should have a form of URI-like string like `s3a:///my-bucket/my-prefix` or `file:///home/user/`.
</span></span></span><span class="line"><span class="cl"><span class="s2">      &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="n">hadoop</span><span class="p">,</span> <span class="n">hdfs</span><span class="p">,</span> <span class="n">fs_type</span> <span class="o">=</span> <span class="n">_get_hdfs</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">_hdfs</span> <span class="o">=</span> <span class="n">hdfs</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">_fs_type</span> <span class="o">=</span> <span class="n">fs_type</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">_hadoop</span> <span class="o">=</span> <span class="n">hadoop</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_jvm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">write_utf8</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="p">:</span> <span class="s2">&#34;HadoopFileSystem&#34;</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&#34;a&#34;</span><span class="p">,</span> <span class="s2">&#34;w&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;&#34;&#34;Write a given string in UTF-16BE to the given path.
</span></span></span><span class="line"><span class="cl"><span class="s2">      Do not use this method to write the data!
</span></span></span><span class="line"><span class="cl"><span class="s2">      It is fantastically slow compared to `spark.write`.
</span></span></span><span class="line"><span class="cl"><span class="s2">      :param path: Path of file
</span></span></span><span class="line"><span class="cl"><span class="s2">      :param data: String to write
</span></span></span><span class="line"><span class="cl"><span class="s2">      :param mode: Mode. `w` means overwrite but `a` means append.
</span></span></span><span class="line"><span class="cl"><span class="s2">      &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;w&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># org.apache.hadoop.fs.FileSystem.create(Path f, boolean overwrite)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hdfs</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">      <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;a&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># org.apache.hadoop.fs.FileSystem.append(Path f)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hdfs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># org.apache.hadoop.fs.FSDataOutputStream</span>
</span></span><span class="line"><span class="cl">      <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&#34;utf-8&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">          <span class="n">output_stream</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_stream</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">read_utf8</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="s2">&#34;HadoopFileSystem&#34;</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="s2">&#34;&#34;&#34;Read string from given path.
</span></span></span><span class="line"><span class="cl"><span class="s2">      Do not use this method to read the data!
</span></span></span><span class="line"><span class="cl"><span class="s2">      It is fantastically slow compared to `spark.read`.
</span></span></span><span class="line"><span class="cl"><span class="s2">      :param path: Path of file
</span></span></span><span class="line"><span class="cl"><span class="s2">      :return: Decoded from UTF-8 string
</span></span></span><span class="line"><span class="cl"><span class="s2">      :rtype: str
</span></span></span><span class="line"><span class="cl"><span class="s2">      &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># org.apache.hadoop.fs.FileSystem.open</span>
</span></span><span class="line"><span class="cl">      <span class="n">in_stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hdfs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># open returns us org.apache.hadoop.fs.FSDataInputStream</span>
</span></span><span class="line"><span class="cl">      <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="k">if</span> <span class="n">in_stream</span><span class="o">.</span><span class="n">available</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">in_stream</span><span class="o">.</span><span class="n">readByte</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">          <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">in_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="nb">bytes</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&#34;utf-8&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">glob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">HDFSFile</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">      <span class="n">statuses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hdfs</span><span class="o">.</span><span class="n">globStatus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hadoop</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">pattern</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">file_status</span> <span class="ow">in</span> <span class="n">statuses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># org.apache.hadoop.fs.FileStatus</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">HDFSFile</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">name</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">getPath</span><span class="p">()</span><span class="o">.</span><span class="n">getName</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">path</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">getPath</span><span class="p">()</span><span class="o">.</span><span class="n">toString</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">mod_time</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">getModificationTime</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">is_dir</span><span class="o">=</span><span class="n">file_status</span><span class="o">.</span><span class="n">isDirectory</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">fs_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fs_type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">res</span></span></span></code></pre></div>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-13" class="outline-3">
<h3 id="headline-13">
Conclusion
</h3>
<div id="outline-text-headline-13" class="outline-text-3">
<p>
There is a nice lightweight Python library with zero additional dependencies: <a href="https://github.com/MrPowers/eren">Eren</a>. This library contains a lot of useful routines for working with Hive and Hadoop. I pushed the code above into this library so you are free to use it. All that you need is just to write:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="kn">from</span> <span class="nn">eren</span> <span class="kn">import</span> <span class="n">fs</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">hdfs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">HadoopFileSystem</span><span class="p">(</span><span class="n">spark_session</span><span class="p">,</span> <span class="s2">&#34;hdfs://some-place&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">s3fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">HadoopFileSystem</span><span class="p">(</span><span class="n">spark_session</span><span class="p">,</span> <span class="s2">&#34;s3a://prefix/bucket&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">local_fs</span> <span class="o">=</span> <span class="n">fs</span><span class="o">.</span><span class="n">HadoopFileSystem</span><span class="p">(</span><span class="n">spark_session</span><span class="p">,</span> <span class="s2">&#34;file://my-home-folder&#34;</span><span class="p">)</span></span></span></code></pre></div>
</div>
</div>
</div>
</div>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://semyonsinchenko.gihub.io/ssinchenko/tags/pyspark/">Pyspark</a></li>
      <li><a href="https://semyonsinchenko.gihub.io/ssinchenko/tags/spark/">Spark</a></li>
      <li><a href="https://semyonsinchenko.gihub.io/ssinchenko/tags/hadoop/">Hadoop</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://semyonsinchenko.gihub.io/ssinchenko/post/generating-docs-with-gpt/">
    <span class="title">Â« Prev</span>
    <br>
    <span>Generating docstrings with GPT</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on x"
            href="https://x.com/intent/tweet/?text=Working%20With%20File%20System%20from%20PySpark&amp;url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f&amp;hashtags=pyspark%2cspark%2chadoop">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f&amp;title=Working%20With%20File%20System%20from%20PySpark&amp;summary=Working%20With%20File%20System%20from%20PySpark&amp;source=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f&title=Working%20With%20File%20System%20from%20PySpark">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on whatsapp"
            href="https://api.whatsapp.com/send?text=Working%20With%20File%20System%20from%20PySpark%20-%20https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on telegram"
            href="https://telegram.me/share/url?text=Working%20With%20File%20System%20from%20PySpark&amp;url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Working With File System from PySpark on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Working%20With%20File%20System%20from%20PySpark&u=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fworking-with-fs-pyspark%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://semyonsinchenko.gihub.io/ssinchenko/">Sem Sinchenko</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
